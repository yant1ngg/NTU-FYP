{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8w_-3v5UdDZ"
      },
      "outputs": [],
      "source": [
        "!pip install lipd cartopy PyWavelets scikit-learn eofs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac7732a2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import seaborn as sns\n",
        "import os\n",
        "import lipd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pywt\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "import random\n",
        "from eofs.standard import Eof\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import json\n",
        "\n",
        "# OPTIONAL: Load data from local or online source\n",
        "# If running on Colab and using Google Drive, uncomment below:\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir('/content/drive/MyDrive/your_project_folder')\n",
        "\n",
        "# Otherwise, place your data in the same folder or set the correct relative path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e53d9ed0"
      },
      "outputs": [],
      "source": [
        "with open('bam_results_normalized.json') as f:\n",
        "    bam_results = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5af65b19"
      },
      "outputs": [],
      "source": [
        "# Clone DINEOF repo (do this only once)\n",
        "!git clone https://github.com/zhouweichen1992110/DINEOF-Python.git\n",
        "%cd DINEOF-Python\n",
        "\n",
        "from utils import eof_core, init_missing, normalize, denormalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc0742fc"
      },
      "outputs": [],
      "source": [
        "# üöÄ Functions for Workflow\n",
        "# -------------------------------------------\n",
        "def filter_by_archive_type(bam_results, archive_types):\n",
        "    filtered_results = {}\n",
        "\n",
        "    for k, v in bam_results.items():\n",
        "        archive_type = v.get('archiveType', '')\n",
        "\n",
        "        # Handle list or str\n",
        "        if isinstance(archive_type, list):\n",
        "            archive_type = archive_type[0]\n",
        "\n",
        "        if isinstance(archive_type, str) and archive_type.lower() in [a.lower() for a in archive_types]:\n",
        "            filtered_results[k] = v\n",
        "\n",
        "    print(f\"‚úÖ Total {archive_types} records found: {len(filtered_results)}\")\n",
        "    return filtered_results\n",
        "\n",
        "def get_year_range_from_filtered_results(filtered_results):\n",
        "    all_years = []\n",
        "\n",
        "    for rec_id, rec_data in filtered_results.items():\n",
        "        years = rec_data['bam_output']['tp']\n",
        "        all_years.extend(years)\n",
        "\n",
        "    if not all_years:\n",
        "        print(\"‚ö†Ô∏è No years found in filtered results!\")\n",
        "        return None, None\n",
        "\n",
        "    start_year = int(min(all_years))\n",
        "    end_year = int(max(all_years))\n",
        "\n",
        "    print(f\"üìÖ Records cover from {start_year} to {end_year}\")\n",
        "    return start_year, end_year\n",
        "\n",
        "def sample_one_realization(results_dict):\n",
        "    realization_records = []\n",
        "\n",
        "    for rec_id, rec_data in results_dict.items():\n",
        "        bam_output = rec_data['bam_output']\n",
        "        tp = np.array(bam_output['tp'])\n",
        "        Xp = np.array(bam_output['Xp_normalized'])\n",
        "        n_ensembles = Xp.shape[1]\n",
        "        random_ens_idx = random.randint(0, n_ensembles - 1)\n",
        "\n",
        "        # ‚úÖ Step 1: Grab the archiveType from rec_data\n",
        "        archive_type = rec_data.get('archiveType', '')\n",
        "\n",
        "        # ‚úÖ Step 2: If it's a list (e.g., ['coral']), take the first element\n",
        "        if isinstance(archive_type, list):\n",
        "            archive_type = archive_type[0]\n",
        "\n",
        "        # ‚úÖ Step 3: Add it into the dataframe\n",
        "        series_df = pd.DataFrame({\n",
        "            'Year': tp,\n",
        "            'Value': Xp[:, random_ens_idx],\n",
        "            'ID': rec_id,\n",
        "            'archiveType': archive_type  # ‚úÖ Correct archiveType!\n",
        "        })\n",
        "\n",
        "        realization_records.append(series_df)\n",
        "\n",
        "    realization_df = pd.concat(realization_records, ignore_index=True)\n",
        "    return realization_df\n",
        "\n",
        "def bin_and_aggregate(df, year_start, year_end, bin_size=5):\n",
        "    # Step 1: Create the complete bin range\n",
        "    all_bins = np.arange(year_start, year_end + bin_size, bin_size)\n",
        "\n",
        "    # Step 2: Perform binning\n",
        "    df['Year_Bin'] = ((df['Year'] - year_start) // bin_size) * bin_size + year_start\n",
        "    aggregated_df = df.groupby(['Year_Bin', 'ID']).agg({\n",
        "        'Value': 'mean',\n",
        "        'archiveType': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Step 3: Reindex for each record ID to ensure NaNs for missing bins\n",
        "    complete_df_list = []\n",
        "\n",
        "    for record_id, group in aggregated_df.groupby('ID'):\n",
        "        complete_bins_df = pd.DataFrame({\n",
        "            'Year_Bin': all_bins,\n",
        "            'ID': record_id\n",
        "        })\n",
        "\n",
        "        merged = pd.merge(complete_bins_df, group, on=['Year_Bin', 'ID'], how='left')\n",
        "\n",
        "        # Fill archiveType (if needed)\n",
        "        merged['archiveType'] = merged['archiveType'].fillna(group['archiveType'].iloc[0])\n",
        "\n",
        "        complete_df_list.append(merged)\n",
        "\n",
        "    final_complete_df = pd.concat(complete_df_list, ignore_index=True)\n",
        "\n",
        "    return final_complete_df\n",
        "\n",
        "def build_intervals(year_start, year_end, interval_length, overlap, min_last_interval_length):\n",
        "    step = interval_length - overlap\n",
        "    intervals = []\n",
        "\n",
        "    start = year_start\n",
        "    while start + interval_length - 1 <= year_end:\n",
        "        intervals.append((start, start + interval_length - 1))\n",
        "        start += step\n",
        "\n",
        "    # Handle last interval\n",
        "    if intervals:\n",
        "        last_start = intervals[-1][0]\n",
        "        last_end = intervals[-1][1]\n",
        "\n",
        "        if last_end < year_end:\n",
        "            final_interval = (start, year_end)\n",
        "\n",
        "            # If it's too short, merge with the previous one\n",
        "            if final_interval[1] - final_interval[0] + 1 < min_last_interval_length:\n",
        "                merged = (intervals[-1][0], year_end)\n",
        "                intervals[-1] = merged\n",
        "            else:\n",
        "                intervals.append(final_interval)\n",
        "    else:\n",
        "        # If no intervals were added, just make one full-range interval\n",
        "        intervals.append((year_start, year_end))\n",
        "\n",
        "    return intervals\n",
        "\n",
        "def find_best_interval(final_binned_df, year_start, year_end, bin_size, min_last_interval_length=40, overlap=10):\n",
        "    best_score = -1\n",
        "    best_config = None\n",
        "    results = []\n",
        "\n",
        "    for interval_length in range(50, 201, 10):\n",
        "        intervals = build_intervals(year_start, year_end, interval_length, overlap, min_last_interval_length)\n",
        "\n",
        "        valid_ids_set = set()\n",
        "        interval_valid_counts = {}\n",
        "\n",
        "        for start, end in intervals:\n",
        "            interval_data = final_binned_df[\n",
        "                (final_binned_df['Year_Bin'] >= start) & (final_binned_df['Year_Bin'] <= end)\n",
        "            ]\n",
        "\n",
        "            interval_years = end - start + 1\n",
        "            required_bins = max(1, int((2 / 3) * (interval_years / bin_size)))\n",
        "\n",
        "            valid_ids = {\n",
        "                record_id for record_id, group in interval_data.groupby('ID')\n",
        "                if group['Value'].notna().sum() >= required_bins\n",
        "            }\n",
        "\n",
        "            interval_valid_counts[f\"{start}-{end}\"] = len(valid_ids)\n",
        "            valid_ids_set.update(valid_ids)\n",
        "\n",
        "        num_intervals_with_valid_ids = sum(v > 0 for v in interval_valid_counts.values())\n",
        "        valid_interval_proportion = num_intervals_with_valid_ids / len(intervals)\n",
        "        score = valid_interval_proportion + len(valid_ids_set)\n",
        "\n",
        "        results.append((interval_length, num_intervals_with_valid_ids, len(valid_ids_set), valid_interval_proportion, score))\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = interval_length\n",
        "\n",
        "    results_df = pd.DataFrame(results, columns=[\n",
        "        'Interval_Length', 'Intervals_with_Valid_IDs', 'Unique_IDs', 'Valid_Interval_Proportion', 'Score'\n",
        "    ])\n",
        "\n",
        "    best_row = results_df[results_df['Interval_Length'] == best_config].iloc[0]\n",
        "    print(f\"\\n‚úÖ Best Interval Length: {best_config} years (with {overlap}-year overlap)\")\n",
        "    print(f\" - Max Intervals with Valid IDs: {best_row['Intervals_with_Valid_IDs']}\")\n",
        "    print(f\" - Unique Valid IDs: {best_row['Unique_IDs']}\")\n",
        "    print(f\" - Proportion of Valid Intervals: {best_row['Valid_Interval_Proportion']:.2f}\")\n",
        "    print(f\" - Best Score: {best_row['Score']:.2f}\\n\")\n",
        "\n",
        "    print(\"üèÖ Top 5 Interval Lengths by Score:\")\n",
        "    print(results_df.sort_values(by='Score', ascending=False).head(5))\n",
        "\n",
        "    return best_config, results_df\n",
        "\n",
        "def summarize_interval_coverage(\n",
        "    final_binned_df,\n",
        "    selected_interval_length,\n",
        "    year_start,\n",
        "    year_end,\n",
        "    bin_size,\n",
        "    min_last_interval_length=40,\n",
        "    include_empty_intervals=True,\n",
        "    overlap=10\n",
        "):\n",
        "    intervals = build_intervals(year_start, year_end, selected_interval_length, overlap, min_last_interval_length)\n",
        "\n",
        "    coverage_results = []\n",
        "    interval_counts_dict = {}\n",
        "\n",
        "    for start, end in intervals:\n",
        "        interval_label = f\"{start}-{end}\"\n",
        "        interval_data = final_binned_df[\n",
        "            (final_binned_df['Year_Bin'] >= start) & (final_binned_df['Year_Bin'] <= end)\n",
        "        ]\n",
        "\n",
        "        interval_years = end - start + 1\n",
        "        required_bins = max(1, int((2 / 3) * (interval_years / bin_size)))\n",
        "\n",
        "        valid_ids = {\n",
        "            record_id for record_id, group in interval_data.groupby('ID')\n",
        "            if group['Value'].notna().sum() >= required_bins\n",
        "        }\n",
        "\n",
        "        interval_counts_dict[interval_label] = len(valid_ids)\n",
        "\n",
        "        for record_id in valid_ids:\n",
        "            group = interval_data[interval_data['ID'] == record_id].copy()\n",
        "            group['Interval'] = interval_label\n",
        "            coverage_results.append(group)\n",
        "\n",
        "    interval_valid_df = pd.concat(coverage_results, ignore_index=True) if coverage_results else pd.DataFrame()\n",
        "    total_unique_ids = interval_valid_df['ID'].nunique() if not interval_valid_df.empty else 0\n",
        "\n",
        "    print(f\"\\nüìä Summary for {selected_interval_length}-year Interval with {overlap}-year Overlap\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üîπ Total Unique IDs Across All Intervals: {total_unique_ids}\")\n",
        "    print(f\"üîπ Last Interval (After Merge Check): {intervals[-1][0]}-{intervals[-1][1]}\\n\")\n",
        "\n",
        "    print(\"\\nüìå Unique ID Count Per Interval:\")\n",
        "    for label, count in interval_counts_dict.items():\n",
        "        print(f\"  - {label}: {count} IDs\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "def filter_by_interval_coverage(\n",
        "    final_binned_df,\n",
        "    selected_interval_length,\n",
        "    year_start,\n",
        "    year_end,\n",
        "    bin_size,\n",
        "    min_last_interval_length=40,\n",
        "    overlap=10\n",
        "):\n",
        "    intervals = build_intervals(year_start, year_end, selected_interval_length, overlap, min_last_interval_length)\n",
        "\n",
        "    coverage_results = []\n",
        "\n",
        "    for start, end in intervals:\n",
        "        interval_data = final_binned_df[\n",
        "            (final_binned_df['Year_Bin'] >= start) & (final_binned_df['Year_Bin'] <= end)\n",
        "        ]\n",
        "\n",
        "        interval_years = end - start + 1\n",
        "        required_bins = max(1, int((2 / 3) * (interval_years / bin_size)))\n",
        "\n",
        "        valid_ids = {\n",
        "            record_id for record_id, group in interval_data.groupby('ID')\n",
        "            if group['Value'].notna().sum() >= required_bins\n",
        "        }\n",
        "\n",
        "        for record_id in valid_ids:\n",
        "            group = interval_data[interval_data['ID'] == record_id].copy()\n",
        "            group['Interval'] = f\"{start}-{end}\"\n",
        "            coverage_results.append(group)\n",
        "\n",
        "    return pd.concat(coverage_results, ignore_index=True) if coverage_results else pd.DataFrame()\n",
        "\n",
        "\n",
        "def run_dineof_on_intervals(interval_valid_df, stopping=0.01, rounds=500, bin_size=5):\n",
        "    dineof_results = []\n",
        "\n",
        "    for interval_name, group in interval_valid_df.groupby('Interval'):\n",
        "        pivot_df = group.pivot(index='Year_Bin', columns='ID', values='Value')\n",
        "\n",
        "        # ‚úÖ Ensure time axis is aligned with bin_size\n",
        "        all_years = np.arange(\n",
        "            pivot_df.index.min(),\n",
        "            pivot_df.index.max() + bin_size,\n",
        "            bin_size\n",
        "        )\n",
        "        pivot_df = pivot_df.reindex(all_years)\n",
        "\n",
        "        # ‚úÖ Convert to NumPy array for DINEOF processing\n",
        "        data_matrix = pivot_df.to_numpy()\n",
        "        valid_id_count = pivot_df.shape[1]\n",
        "\n",
        "        # ‚úÖ Case 1: If only one record ID, do linear interpolation\n",
        "        if valid_id_count == 1:\n",
        "            filled_series = pivot_df.iloc[:, 0].interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "            # Convert to long format\n",
        "            filled_long_df = filled_series.reset_index().rename(\n",
        "                columns={'index': 'Year_Bin', pivot_df.columns[0]: 'Value_filled'}\n",
        "            )\n",
        "            filled_long_df['ID'] = pivot_df.columns[0]\n",
        "            filled_long_df['Interval'] = interval_name\n",
        "\n",
        "            dineof_results.append(filled_long_df)\n",
        "\n",
        "        # ‚úÖ Case 2: Multiple records ‚Üí run DINEOF\n",
        "        elif valid_id_count >= 2:\n",
        "            max_possible_eof = min(data_matrix.shape)\n",
        "            best_eof_index = min(2, max_possible_eof - 1)\n",
        "\n",
        "            if max_possible_eof <= 1:\n",
        "                print(f\"‚ö†Ô∏è Not enough data for DINEOF in interval {interval_name}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Normalize and initialize missing values\n",
        "            dataNorm, norm_params = normalize(data_matrix, 'meanrows', 'stdrows')\n",
        "            dataInit, mask = init_missing(dataNorm, 'column')\n",
        "\n",
        "            # Apply DINEOF\n",
        "            filled_data, _, _ = eof_core(\n",
        "                dataInit,\n",
        "                mask,\n",
        "                best_eof_index,\n",
        "                stop_criterion=stopping,\n",
        "                max_iterations=rounds\n",
        "            )\n",
        "\n",
        "            # Denormalize back to original scale\n",
        "            filled_data = denormalize(filled_data, 'std', norm_params[1], 'mean', norm_params[0])\n",
        "\n",
        "            # Convert back to DataFrame\n",
        "            filled_df = pd.DataFrame(filled_data, index=pivot_df.index, columns=pivot_df.columns)\n",
        "            filled_long_df = filled_df.reset_index().melt(\n",
        "                id_vars='Year_Bin',\n",
        "                var_name='ID',\n",
        "                value_name='Value_filled'\n",
        "            )\n",
        "            filled_long_df['Interval'] = interval_name\n",
        "\n",
        "            dineof_results.append(filled_long_df)\n",
        "\n",
        "    # ‚úÖ Combine results into one dataframe\n",
        "    dineof_df = pd.concat(dineof_results, ignore_index=True) if dineof_results else pd.DataFrame()\n",
        "\n",
        "    # ‚úÖ Fallback if DINEOF failed completely\n",
        "    if not dineof_df.empty and dineof_df['Value_filled'].isna().all():\n",
        "        print(\"‚ö†Ô∏è DINEOF failed completely! Using raw values instead.\")\n",
        "        dineof_df['Value_filled'] = dineof_df['Value']\n",
        "\n",
        "    return dineof_df\n",
        "\n",
        "def perform_standard_pca(dineof_filled_df, realization_num):\n",
        "    pca_results = []\n",
        "    explained_variances = {}\n",
        "\n",
        "    for interval, group in dineof_filled_df.groupby('Interval'):\n",
        "        pivot_df = group.pivot(index='Year_Bin', columns='ID', values='Value_filled')\n",
        "\n",
        "        # Skip intervals with no valid records\n",
        "        if pivot_df.shape[1] == 0:\n",
        "            continue\n",
        "\n",
        "        # Single-record interval: keep raw values as PC1\n",
        "        if pivot_df.shape[1] == 1:\n",
        "            pca_df = pd.DataFrame({\n",
        "                'Year_Bin': pivot_df.index,\n",
        "                'PC1': pivot_df.iloc[:, 0].values,\n",
        "                'Interval': interval,\n",
        "                'Realization': realization_num\n",
        "            }).reset_index()\n",
        "            pca_results.append(pca_df)\n",
        "            continue\n",
        "\n",
        "        # Standard PCA for multiple records\n",
        "        standardized_data = (pivot_df - pivot_df.mean()) / pivot_df.std()\n",
        "\n",
        "        pca = PCA()\n",
        "        pcs = pca.fit_transform(standardized_data)\n",
        "\n",
        "        explained_variances[interval] = pca.explained_variance_ratio_\n",
        "\n",
        "        pca_df = pd.DataFrame(pcs, columns=[f\"PC{i+1}\" for i in range(pcs.shape[1])])\n",
        "        pca_df['Year_Bin'] = pivot_df.index\n",
        "        pca_df['Interval'] = interval\n",
        "        pca_df['Realization'] = realization_num\n",
        "\n",
        "        pca_results.append(pca_df)\n",
        "\n",
        "    combined_pca_df = pd.concat(pca_results, ignore_index=True) if pca_results else pd.DataFrame()\n",
        "    combined_pca_df = combined_pca_df.sort_values(by='Year_Bin')\n",
        "\n",
        "    return combined_pca_df, explained_variances\n",
        "\n",
        "\n",
        "def perform_kernel_pca(dineof_filled_df, realization_num, gamma=0.1):\n",
        "    kernel_pca_results = []\n",
        "    explained_variances = {}\n",
        "\n",
        "    for interval, group in dineof_filled_df.groupby('Interval'):\n",
        "        pivot_df = group.pivot(index='Year_Bin', columns='ID', values='Value_filled')\n",
        "\n",
        "        # Skip intervals with no valid records\n",
        "        if pivot_df.shape[1] == 0:\n",
        "            continue\n",
        "\n",
        "        # Single-record interval: keep raw values as KPC1\n",
        "        if pivot_df.shape[1] == 1:\n",
        "            kpca_df = pivot_df.reset_index()[['Year_Bin']]\n",
        "            kpca_df['KPC1'] = pivot_df.iloc[:, 0].values\n",
        "            kpca_df['Interval'] = interval\n",
        "            kpca_df['Realization'] = realization_num\n",
        "            kernel_pca_results.append(kpca_df)\n",
        "            continue\n",
        "\n",
        "        # Kernel PCA for multiple records\n",
        "        standardized_data = (pivot_df - pivot_df.mean()) / pivot_df.std()\n",
        "\n",
        "        kpca = KernelPCA(\n",
        "            n_components=min(standardized_data.shape),\n",
        "            kernel='rbf',\n",
        "            gamma=gamma,\n",
        "            fit_inverse_transform=True\n",
        "        )\n",
        "        pcs = kpca.fit_transform(standardized_data)\n",
        "\n",
        "        # Manually calculate explained variance ratio\n",
        "        K = rbf_kernel(standardized_data, gamma=gamma)\n",
        "        n_samples = K.shape[0]\n",
        "        one_n = np.ones((n_samples, n_samples)) / n_samples\n",
        "        K_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n",
        "\n",
        "        eigvals, _ = np.linalg.eigh(K_centered)\n",
        "        eigvals_sorted = np.sort(eigvals)[::-1]\n",
        "\n",
        "        explained_variance_ratio = eigvals_sorted / eigvals_sorted.sum()\n",
        "        explained_variances[interval] = explained_variance_ratio\n",
        "\n",
        "        kpca_df = pd.DataFrame(pcs, columns=[f\"KPC{i+1}\" for i in range(pcs.shape[1])])\n",
        "        kpca_df['Year_Bin'] = pivot_df.index\n",
        "        kpca_df['Interval'] = interval\n",
        "        kpca_df['Realization'] = realization_num\n",
        "\n",
        "        kernel_pca_results.append(kpca_df)\n",
        "\n",
        "    combined_kpca_df = pd.concat(kernel_pca_results, ignore_index=True) if kernel_pca_results else pd.DataFrame()\n",
        "    combined_kpca_df = combined_kpca_df.sort_values(by='Year_Bin')\n",
        "\n",
        "    return combined_kpca_df, explained_variances\n",
        "\n",
        "\n",
        "def perform_wavelet_pca(dineof_filled_df, realization_num, wavelet='db4', level=1):\n",
        "    wavelet_pca_results = []\n",
        "    explained_variances = {}\n",
        "\n",
        "    for interval, group in dineof_filled_df.groupby('Interval'):\n",
        "        pivot_df = group.pivot(index='Year_Bin', columns='ID', values='Value_filled')\n",
        "\n",
        "        # Skip intervals with no valid records\n",
        "        if pivot_df.shape[1] == 0:\n",
        "            continue\n",
        "\n",
        "        # Single-record interval: keep raw values as WPC1\n",
        "        if pivot_df.shape[1] == 1:\n",
        "            wavelet_pca_df = pivot_df.reset_index()[['Year_Bin']]\n",
        "            wavelet_pca_df['WPC1'] = pivot_df.iloc[:, 0].values\n",
        "            wavelet_pca_df['Interval'] = interval\n",
        "            wavelet_pca_df['Realization'] = realization_num\n",
        "            wavelet_pca_results.append(wavelet_pca_df)\n",
        "            continue\n",
        "\n",
        "        # Wavelet PCA for multiple records\n",
        "        wavelet_filtered = []\n",
        "        for col in pivot_df.columns:\n",
        "            ts = pivot_df[col].values\n",
        "            if np.isnan(ts).any():\n",
        "                continue\n",
        "\n",
        "            coeffs = pywt.wavedec(ts, wavelet, level=level)\n",
        "            coeffs_filtered = [coeffs[0]] + [np.zeros_like(c) for c in coeffs[1:]]\n",
        "            reconstructed_ts = pywt.waverec(coeffs_filtered, wavelet)[:len(ts)]\n",
        "            wavelet_filtered.append(reconstructed_ts)\n",
        "\n",
        "        wavelet_matrix = np.array(wavelet_filtered).T\n",
        "\n",
        "        standardized_data = (wavelet_matrix - wavelet_matrix.mean(axis=0)) / wavelet_matrix.std(axis=0)\n",
        "\n",
        "        pca = PCA()\n",
        "        pcs = pca.fit_transform(standardized_data)\n",
        "\n",
        "        explained_variances[interval] = pca.explained_variance_ratio_\n",
        "\n",
        "        wavelet_pca_df = pd.DataFrame(pcs, columns=[f\"WPC{i+1}\" for i in range(pcs.shape[1])])\n",
        "        wavelet_pca_df['Year_Bin'] = pivot_df.index\n",
        "        wavelet_pca_df['Interval'] = interval\n",
        "        wavelet_pca_df['Realization'] = realization_num\n",
        "\n",
        "        wavelet_pca_results.append(wavelet_pca_df)\n",
        "\n",
        "    combined_wavelet_pca_df = pd.concat(wavelet_pca_results, ignore_index=True) if wavelet_pca_results else pd.DataFrame()\n",
        "    combined_wavelet_pca_df = combined_wavelet_pca_df.sort_values(by='Year_Bin')\n",
        "\n",
        "    return combined_wavelet_pca_df, explained_variances\n",
        "\n",
        "def set_global_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def run_archive_realizations(\n",
        "    archive_results,\n",
        "    n_realizations=200,\n",
        "    selected_interval_length=None,\n",
        "    year_start=None,\n",
        "    year_end=None,\n",
        "    bin_size=5,\n",
        "    seed_value=42,\n",
        "    overlap=10  # ‚úÖ New: Overlap parameter\n",
        "):\n",
        "    set_global_seed(seed_value)\n",
        "\n",
        "    if year_start is None or year_end is None:\n",
        "        raise ValueError(\"year_start and year_end must be specified!\")\n",
        "\n",
        "    interval_length_to_use = selected_interval_length\n",
        "\n",
        "    all_pca_results = []\n",
        "    all_kpca_results = []\n",
        "    all_wavelet_pca_results = []\n",
        "\n",
        "    all_pca_evrs = []\n",
        "    all_kpca_evrs = []\n",
        "    all_wavelet_pca_evrs = []\n",
        "\n",
        "    for realization_num in range(1, n_realizations + 1):\n",
        "        set_global_seed(seed_value + realization_num)\n",
        "\n",
        "        print(f\"\\nüé≤ Realization {realization_num}/{n_realizations} | Interval Length: {interval_length_to_use} years | Overlap: {overlap} years\")\n",
        "\n",
        "        sampled_df = sample_one_realization(archive_results)\n",
        "\n",
        "        binned_results = [\n",
        "            bin_and_aggregate(group, year_start=year_start, year_end=year_end, bin_size=bin_size)\n",
        "            for record_id, group in sampled_df.groupby('ID')\n",
        "        ]\n",
        "        final_binned_df = pd.concat(binned_results, ignore_index=True)\n",
        "\n",
        "        interval_valid_df = filter_by_interval_coverage(\n",
        "            final_binned_df,\n",
        "            selected_interval_length=interval_length_to_use,\n",
        "            year_start=year_start,\n",
        "            year_end=year_end,\n",
        "            bin_size=bin_size,\n",
        "            overlap=overlap  # ‚úÖ Propagate overlap\n",
        "        )\n",
        "\n",
        "        if interval_valid_df.empty:\n",
        "            print(\"‚ö†Ô∏è No valid data after interval filtering. Skipping realization.\")\n",
        "            continue\n",
        "\n",
        "        dineof_filled_df = run_dineof_on_intervals(interval_valid_df, bin_size=bin_size)\n",
        "\n",
        "        if dineof_filled_df.empty:\n",
        "            print(\"‚ö†Ô∏è DINEOF failed to fill data. Skipping realization.\")\n",
        "            continue\n",
        "\n",
        "        pca_df, pca_evrs = perform_standard_pca(dineof_filled_df, realization_num)\n",
        "        all_pca_results.append(pca_df)\n",
        "        all_pca_evrs.append(pca_evrs)\n",
        "\n",
        "        kpca_df, kpca_evrs = perform_kernel_pca(dineof_filled_df, realization_num)\n",
        "        all_kpca_results.append(kpca_df)\n",
        "        all_kpca_evrs.append(kpca_evrs)\n",
        "\n",
        "        wavelet_df, wavelet_evrs = perform_wavelet_pca(dineof_filled_df, realization_num)\n",
        "        all_wavelet_pca_results.append(wavelet_df)\n",
        "        all_wavelet_pca_evrs.append(wavelet_evrs)\n",
        "\n",
        "        print(f\"‚úÖ Finished Realization {realization_num}\")\n",
        "\n",
        "    return {\n",
        "        \"pca_results\": all_pca_results,\n",
        "        \"kpca_results\": all_kpca_results,\n",
        "        \"wavelet_pca_results\": all_wavelet_pca_results,\n",
        "        \"pca_evrs\": all_pca_evrs,\n",
        "        \"kpca_evrs\": all_kpca_evrs,\n",
        "        \"wavelet_pca_evrs\": all_wavelet_pca_evrs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_results = bam_results\n",
        "\n",
        "year_start = 1551\n",
        "year_end = 2016\n",
        "bin_size = 1\n",
        "overlap = 10\n",
        "\n",
        "realization_df = sample_one_realization(filtered_results)\n",
        "binned_df = bin_and_aggregate(realization_df, year_start=year_start, year_end=year_end, bin_size=bin_size)\n",
        "best_interval, interval_results_df = find_best_interval(\n",
        "    binned_df,\n",
        "    year_start=year_start,\n",
        "    year_end=year_end,\n",
        "    bin_size=bin_size,\n",
        "    overlap=overlap\n",
        ")"
      ],
      "metadata": {
        "id": "hvs7Elt6e6iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_interval_length = 150\n",
        "\n",
        "summarize_interval_coverage(\n",
        "    final_binned_df=binned_df,\n",
        "    selected_interval_length=selected_interval_length,\n",
        "    year_start=year_start,\n",
        "    year_end=year_end,\n",
        "    bin_size=bin_size,\n",
        "    include_empty_intervals=True,\n",
        "    overlap=overlap\n",
        ")\n",
        "\n",
        "interval_valid_df = filter_by_interval_coverage(\n",
        "    binned_df,\n",
        "    selected_interval_length=selected_interval_length,\n",
        "    year_start=year_start,\n",
        "    year_end=year_end,\n",
        "    bin_size=bin_size,\n",
        "    overlap=overlap\n",
        ")"
      ],
      "metadata": {
        "id": "FTDRvu_Yv4QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_realizations = 200\n",
        "seed_value = 42\n",
        "set_global_seed(seed_value)\n",
        "\n",
        "\n",
        "results = run_archive_realizations(\n",
        "    archive_results=filtered_results,\n",
        "    n_realizations=n_realizations,\n",
        "    selected_interval_length=selected_interval_length,\n",
        "    year_start=year_start,\n",
        "    year_end=year_end,\n",
        "    bin_size=bin_size,\n",
        "    seed_value=seed_value,\n",
        "    overlap=overlap\n",
        ")"
      ],
      "metadata": {
        "id": "PPQnKoLRYXPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_signs_for_realizations(results_list, pc_column='PC1'):\n",
        "    aligned_results = []\n",
        "\n",
        "    for realization_idx, realization_df in enumerate(results_list):\n",
        "        print(f\"\\nüß† Processing Realization {realization_idx + 1}\")\n",
        "        df = realization_df.copy()\n",
        "\n",
        "        interval_order = sorted(df['Interval'].unique(), key=lambda x: int(x.split('-')[0]), reverse=True)\n",
        "\n",
        "        # Initialize reference with the most recent interval\n",
        "        ref_interval = interval_order[0]\n",
        "        ref_df = df[df['Interval'] == ref_interval].set_index('Year_Bin')\n",
        "        print(f\"üìå Reference interval: {ref_interval}\")\n",
        "\n",
        "        for i in range(1, len(interval_order)):\n",
        "            curr_interval = interval_order[i]\n",
        "            curr_df = df[df['Interval'] == curr_interval].set_index('Year_Bin')\n",
        "\n",
        "            overlap_years = ref_df.index.intersection(curr_df.index)\n",
        "            overlap_years = sorted(overlap_years)\n",
        "\n",
        "            print(f\"\\nüîç Comparing {curr_interval} against reference {ref_interval}\")\n",
        "            print(f\"   Overlapping years: {overlap_years}\")\n",
        "\n",
        "            if len(overlap_years) >= 3:\n",
        "                ref_values = ref_df.loc[overlap_years, pc_column].values\n",
        "                curr_values = curr_df.loc[overlap_years, pc_column].values\n",
        "\n",
        "                print(f\"   {ref_interval} {pc_column} values: {ref_values}\")\n",
        "                print(f\"   {curr_interval} {pc_column} values: {curr_values}\")\n",
        "\n",
        "                corr = np.corrcoef(ref_values, curr_values)[0, 1]\n",
        "                print(f\"   Correlation: {corr:.3f}\")\n",
        "\n",
        "                if not np.isnan(corr) and corr < 0:\n",
        "                    print(f\"   üîÅ Flipping signs for {curr_interval}\")\n",
        "                    df.loc[df['Interval'] == curr_interval, pc_column] *= -1\n",
        "                else:\n",
        "                    print(f\"   ‚úÖ No flip needed for {curr_interval}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è Not enough overlapping years ({len(overlap_years)}). Skipping.\")\n",
        "\n",
        "            # ‚úÖ Update reference for next iteration\n",
        "            ref_interval = curr_interval\n",
        "            ref_df = df[df['Interval'] == curr_interval].set_index('Year_Bin')\n",
        "\n",
        "        aligned_results.append(df)\n",
        "\n",
        "    return aligned_results\n",
        "\n",
        "\n",
        "def stitch_intervals_keep_recent(df_list, pc_column='PC1'):\n",
        "    stitched_records = []\n",
        "\n",
        "    for df in df_list:\n",
        "        # Sort intervals from most recent to oldest\n",
        "        interval_order = sorted(df['Interval'].unique(), key=lambda x: int(x.split('-')[0]), reverse=True)\n",
        "\n",
        "        # Track which years have already been used\n",
        "        used_years = set()\n",
        "        records = []\n",
        "\n",
        "        for interval in interval_order:\n",
        "            interval_df = df[df['Interval'] == interval].copy()\n",
        "\n",
        "            # Only keep rows with Year_Bin not already used\n",
        "            interval_df = interval_df[~interval_df['Year_Bin'].isin(used_years)]\n",
        "\n",
        "            # Record used years\n",
        "            used_years.update(interval_df['Year_Bin'].tolist())\n",
        "\n",
        "            records.append(interval_df[['Year_Bin', 'Realization', pc_column]])\n",
        "\n",
        "        combined_df = pd.concat(records, ignore_index=True)\n",
        "        stitched_records.append(combined_df)\n",
        "\n",
        "    final_df = pd.concat(stitched_records, ignore_index=True)\n",
        "    final_df = final_df.sort_values(by=['Realization', 'Year_Bin'])\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def normalize_column_by_realization(df, value_col):\n",
        "    df[value_col + '_norm'] = df.groupby('Realization')[value_col].transform(\n",
        "        lambda x: (x - x.mean()) / x.std(ddof=0)\n",
        "    )\n",
        "    return df"
      ],
      "metadata": {
        "id": "yz62xDqQoV7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_pca = flip_signs_for_realizations(results['pca_results'], pc_column='PC1')\n",
        "aligned_kpca = flip_signs_for_realizations(results['kpca_results'], pc_column='KPC1')\n",
        "aligned_wpca = flip_signs_for_realizations(results['wavelet_pca_results'], pc_column='WPC1')"
      ],
      "metadata": {
        "id": "2fvKuwGtQNTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stitched_pca_df = stitch_intervals_keep_recent(aligned_pca, pc_column='PC1')\n",
        "stitched_kpca_df = stitch_intervals_keep_recent(aligned_kpca, pc_column='KPC1')\n",
        "stitched_wpca_df = stitch_intervals_keep_recent(aligned_wpca, pc_column='WPC1')\n",
        "\n",
        "stitched_pca_df = normalize_column_by_realization(stitched_pca_df, 'PC1')\n",
        "stitched_kpca_df = normalize_column_by_realization(stitched_kpca_df, 'KPC1')\n",
        "stitched_wpca_df = normalize_column_by_realization(stitched_wpca_df, 'WPC1')"
      ],
      "metadata": {
        "id": "SIVef9UIoaxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and compute percentiles\n",
        "merged = pd.DataFrame({'Year_Bin': stitched_pca_df['Year_Bin'].unique()})\n",
        "for df, label in zip(\n",
        "    [stitched_pca_df, stitched_kpca_df, stitched_wpca_df],\n",
        "    ['PC1_norm', 'KPC1_norm', 'WPC1_norm']\n",
        "):\n",
        "    temp = df.groupby('Year_Bin')[label].quantile([0.05, 0.5, 0.95]).unstack()\n",
        "    temp.columns = [f\"{label}_p5\", f\"{label}_p50\", f\"{label}_p95\"]\n",
        "    merged = pd.merge(merged, temp, left_on='Year_Bin', right_index=True, how='left')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 8))\n",
        "for label, color in zip(['PC1_norm', 'KPC1_norm', 'WPC1_norm'], ['red', 'green', 'blue']):\n",
        "    x = merged['Year_Bin'].astype(float).values\n",
        "    y_median = merged[f'{label}_p50'].astype(float).values\n",
        "    y_low = merged[f'{label}_p5'].astype(float).values\n",
        "    y_high = merged[f'{label}_p95'].astype(float).values\n",
        "\n",
        "    plt.plot(x, y_median, label=label.split('_')[0] + ' Median', color=color, linewidth=2)\n",
        "    plt.fill_between(x, y_low, y_high, color=color, alpha=0.2)\n",
        "\n",
        "plt.title('Median and 5th‚Äì95th Percentile Range of PC1 from PCA, Kernel PCA, and Wavelet PCA')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Normalized PC1 Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nxh6a4Ucqqf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Extract explained variance ratios from results dictionary\n",
        "all_pca_evrs = results['pca_evrs']\n",
        "all_kpca_evrs = results['kpca_evrs']\n",
        "all_wavelet_pca_evrs = results['wavelet_pca_evrs']\n",
        "\n",
        "# ‚úÖ Define a helper function to compute mean explained variance for PC1\n",
        "def compute_mean_pc1_variance(evrs_list, method_name='PCA'):\n",
        "    pc1_variances = []\n",
        "\n",
        "    for evr_dict in evrs_list:\n",
        "        for interval, evrs in evr_dict.items():\n",
        "            if isinstance(evrs, (list, np.ndarray)) and len(evrs) > 0:\n",
        "                pc1_variances.append(evrs[0])\n",
        "\n",
        "    if pc1_variances:\n",
        "        mean_pc1_variance = np.mean(pc1_variances)\n",
        "        print(f\"‚úÖ Mean explained variance ratio for {method_name} (Component 1): {mean_pc1_variance:.2%}\")\n",
        "        return mean_pc1_variance\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è No explained variance data found for {method_name}.\")\n",
        "        return np.nan\n",
        "\n",
        "# ‚úÖ Run for each PCA method\n",
        "mean_pc1_variance   = compute_mean_pc1_variance(all_pca_evrs, method_name='Standard PCA')\n",
        "mean_kpc1_variance  = compute_mean_pc1_variance(all_kpca_evrs, method_name='Kernel PCA')\n",
        "mean_wpc1_variance  = compute_mean_pc1_variance(all_wavelet_pca_evrs, method_name='Wavelet PCA')\n"
      ],
      "metadata": {
        "id": "EnxCQlpArlGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}